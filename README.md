# ğŸ›¡ï¸ Real-Time Toxic Content Detector

A powerful, modular AI system to detect **toxic text** and **NSFW images** in real time. Built for hackathons and production-ready environments to help moderate content across websites, apps, and social platforms.

## ğŸŒ Live API Deployment

**Render URL:** [https://toxic-detector-ai.onrender.com](https://toxic-detector-ai.onrender.com)  
(You can integrate this API into any frontend/backend application.)

---

## ğŸš€ Features

- âœ… Real-time **toxic text detection** using `DistilBERT` or `Toxic-BERT`
- ğŸ–¼ï¸ **NSFW image classification** using a MobileNetV2 CNN model
- âš¡ FastAPI backend for high-performance REST APIs
- ğŸ”Œ Ready to plug into any frontend (React, Next.js, Vue, etc.)
- ğŸ“¦ Easily deployable to **Render**, **Railway**, or any cloud provider

---

## ğŸ§  AI Models

| Module        | Model                            | Library      |
|---------------|----------------------------------|--------------|
| Text Analysis | `unitary/toxic-bert` (HF Model)  | ğŸ¤— Transformers |
| Image Analysis| Custom MobileNetV2               | TensorFlow / Keras |

---

## ğŸ“ Project Structure
toxic-detector-ai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py             
â”‚   â”œâ”€â”€ utils.py            
â”‚   â”œâ”€â”€ model_image.py
â”‚   â”œâ”€â”€ model_text.py 
â”‚   â””â”€â”€ schemas.py 
â”‚           
â”œâ”€â”€ requirements.txt                       
â”œâ”€â”€ README.md   
